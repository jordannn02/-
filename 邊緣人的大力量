import defSim as ds
import numpy as np
from scipy.stats import poisson
from networkx import expected_degree_graph
import networkx as nx
# from defSim.tools.CreateOutputTable import create_output_table
from Simulation import Simulation
from Utils import directed_spatial_random_graph, plot_degree_dist, \
    create_output_table, get_node_ids, set_nodes, get_net_info
import pandas as pd
import copy
import os
import json
from multiprocessing import Process, Pool
import itertools

np.random.seed(0)
N = 500
K = 12  # out degree number
steps = 100000
exp_nums = 30
output_dir = 'outputs'

N=500
K=12 # degree number

#設定檔案儲存路徑
file_path = r''

for i in range(30):
    #重新生成網絡
    G_poisson = directed_spatial_random_graph(
        num_agents=N, 
        degree=[round(i) for i in (np.random.poisson(K,N) * np.random.poisson(K,N)) / K],
        return_positions = False 
    ) 
    
    #計算degree
    node_degrees = dict(G_poisson.degree())
    sorted_degrees = sorted(node_degrees.items(), key=lambda x: x[1], reverse=True)
 
    #計算clustering
    clustering_dict = nx.clustering(G_poisson)
    
   #取出最高和最低25名的指標
    top_25_degree = sorted_degrees[:25]
    low_25_degree = sorted_degrees[-25:]
    
    top_25_constraint = [(k,v) for k, v in constraint_dict.items() if k in [i[0] for i in top_25_degree]]
    low_25_constraint = [(k,v) for k, v in constraint_dict.items() if k in [i[0] for i in low_25_degree]]

    top_25_clustering = [(k,v) for k, v in clustering_dict.items() if k in [i[0] for i in top_25_degree]]
    low_25_clustering = [(k,v) for k, v in clustering_dict.items() if k in [i[0] for i in low_25_degree]]
    
    #建立檔案
    csvFile = open(os.path.join(file_path, 'random_graph_exp' + str(i+1) + '.csv'), 'w',newline='')
    writer = csv.writer(csvFile)
    
    #寫入最高25名
    for n, d in top_25_degree:
        row = [n, d, redundancy_dict[n], constraint_dict[n], clustering_dict[n]]
        writer.writerow(row)
        
   #寫入最低25名
    for n, d in low_25_degree:
        row = [n, d, redundancy_dict[n], constraint_dict[n], clustering_dict[n]]
        writer.writerow(row)

    csvFile.close()


def initialize_exp():
    deg = [round(i) for i in (np.random.poisson(K, N) * np.random.poisson(K, N)) / K]
    G_poisson = directed_spatial_random_graph(
        num_agents=N,
        degree=deg,
        return_positions=False
    )
    simrun = Simulation(
        seed=555,  # seed for replicability
        network=G_poisson,
        # topology='grid',
        attributes_initializer="random_categorical",  # continuous opinion with random start value
        dissimilarity_measure="hamming",  # distance calculator
        influence_function="similarity_adoption",  # influence
        neighbor_selector='similar',
        stop_condition="strict_convergence",  # it is assumed that little change is possible anymore
        max_iterations=100000,  # simulation stops after this # of ticks
        communication_regime='one-to-many',
        parameter_dict={  # dictionary for all arguments passed to modules
            'n': N,  # size of the network
            'num_features': 3,  # number of opinion features agents posess
            'num_traits': 3
        })

    return simrun


def do_exp(exp_num):
    if not os.path.exists(os.path.join(output_dir, str(exp_num))):
        os.makedirs(os.path.join(output_dir, str(exp_num)))

    net_info_path = os.path.join(output_dir, str(exp_num), 'network_info.txt')
    if os.path.exists(net_info_path):
        return

    simrun_ = initialize_exp()
    info = get_net_info(simrun_.network)

    with open(net_info_path, 'w') as file:
        file.write(json.dumps(info))  # use `json.loads` to do the reverse

    for rumormonger_type in ['all', 'kol', 'loner']:
        csv_path = os.path.join(output_dir, str(exp_num), f'{rumormonger_type}_result.csv')
        simrun = copy.deepcopy(simrun_)
        simrun.initialize()
        result = {}
        itertool_ = itertools.product(["0", "1", "2"], ["0", "1", "2"], ["0", "1", "2", "99"])
        for l in list(itertool_):
            result["Type:" + "_".join(l)] = []
        # select rumormonger
        node_ids = get_node_ids(simrun.network, rumormonger_type)
        for i in range(steps):
            set_nodes(simrun.network, node_ids=node_ids, values={'f01': 1, 'f02': 1, 'f03': 99})
            simrun.run_step()
            set_nodes(simrun.network, node_ids=node_ids, values={'f01': 1, 'f02': 1, 'f03': 99})
            df1 = create_output_table(simrun.network, realizations=['Basic', 'RegionsList', 'ClusterFinder'])
            for key in df1.keys():
                if key not in result.keys():
                    result[key] = []
                result[key].append(df1[key])

            for key in result.keys():
                if 'Type:' in key:
                    result[key].append(0)

            for node_idx in range(len(simrun.network.nodes)):
                key = "Type:" + "_".join(
                    [str(simrun.network.nodes[node_idx][key]) for key in simrun.network.nodes[node_idx].keys()])
                result[key][-1] += 1

        result_pd = pd.DataFrame(result)
        result_pd.to_csv(csv_path)


def make_summary():
    for rumormonger_type in ['all', 'kol', 'loner']:
        dfs = []
        for exp_num in range(exp_nums):
            csv_path = os.path.join(output_dir, str(exp_num), f'{rumormonger_type}_result.csv')
            dfs.append(pd.read_csv(csv_path))

        # table 1
        df1 = []
        for exp_num in range(exp_nums):
            df1.append(dfs[exp_num].iloc[[-1], :])
        df1 = pd.concat(df1, axis=0)
        df1.to_csv(os.path.join(output_dir, f"{rumormonger_type}_table_1.csv"))

        # table 2
        df2 = pd.concat([each._get_numeric_data().stack() for each in dfs], axis=1) \
            .apply(lambda x: x.mean(), axis=1) \
            .unstack()
        df2.to_csv(os.path.join(output_dir, f"{rumormonger_type}_table_2.csv"))

        # table 2
        df2 = pd.concat([each._get_numeric_data().stack() for each in dfs], axis=1) \
            .apply(lambda x: x.mean(), axis=1) \
            .unstack()
        df2.to_csv(os.path.join(output_dir, f"{rumormonger_type}_table_2.csv"))

    # table 3
    dfs = []  # an empty list to store the data frames
    for exp_num in range(exp_nums):
        data = pd.read_json(os.path.join(output_dir, str(exp_num), 'network_info.txt'),
                            lines=True)  # read data frame from json file
        dfs.append(data)  # append the data frame to the list

    df3 = pd.concat(dfs, ignore_index=True)  # concatenate all the data frames in the list.
    df3.to_csv(os.path.join(output_dir, f"table_3.csv"))


if __name__ == "__main__":
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    pool = Pool(4);
    pool.map(do_exp, list(range(exp_nums)))
    make_summary()
